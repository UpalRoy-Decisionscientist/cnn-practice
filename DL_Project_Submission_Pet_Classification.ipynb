{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Project_Submission_Pet_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UpalRoy-Decisionscientist/cnn-practice/blob/master/DL_Project_Submission_Pet_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1STDTCZZW5g6",
        "colab_type": "text"
      },
      "source": [
        "# PROJECT: Pet Classification Tensorflow Model Using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL-K1kzRKb4K",
        "colab_type": "text"
      },
      "source": [
        "Model Developed: Upal Roy\n",
        "Model Date: 6/29/2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcjrxt5iKSt4",
        "colab_type": "text"
      },
      "source": [
        "# Project Objective\n",
        "To build a CNN model that classifies the given pet images correctly into dog and cat images.\n",
        "\n",
        "The project scope document specifies the requirements for the project “Pet Classification Model Using CNN.” Apart from specifying the functional and nonfunctional requirements for the project, it also serves as an input for project scoping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDsKEqt4KZQ9",
        "colab_type": "text"
      },
      "source": [
        "# Project Description and Scope\n",
        "We are provided with the following resources that can be used as inputs for your model:\n",
        "\n",
        "A collection of images of pets, that is​, ​cats and dogs. These images are of different sizes with varied lighting conditions.\n",
        "Code template containing the following code blocks: a. Import modules (part 1) b. Set hyper parameters (part 2) c. Read image data set (part 3) d. Run TensorFlow model (part 4) You are expected to write the code for CNN image classification model (between Parts 3 and 4) using TensorFlow that trains on the data and calculates the accuracy score on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NguHQ6krKseW",
        "colab_type": "text"
      },
      "source": [
        "# Project Guidelines\n",
        "Begin by extracting ipynb file and the data in the same folder. The CNN model (cnn_model_fn) should have the following layers: ● Input layer ● Convolutional layer 1 with 32 filters of kernel size[5,5] ● Pooling layer 1 with pool size**[2,2] **and stride 2 ● Convolutional layer 2 with 64 filters of kernel size[5,5] ● Pooling layer 2 with pool size[2,2] and stride 2 ● Dense layer whose output size is fixed in the hyper parameter: fc_size=32 ● Dropout layer with dropout probability 0.4 Predict the class by doing a softmax on the output of the dropout lay bold text\n",
        "\n",
        "This should be followed by training and evaluation: For the training step, define the loss function and minimize it ● For the evaluation step, calculate the accuracy Run the program for 100, 200, and 300 iterations, respectively. Follow this by a report on the final accuracy and loss on the evaluation data. Prerequisites To execute this project, refer to the installation guide in the downloads section of LMS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6NQX-5ZXj_X",
        "colab_type": "code",
        "outputId": "6100dba2-6034-4c5d-f80c-924c0a62804a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wONr1tOluHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2                 # working with, mainly resizing, images\n",
        "import numpy as np         # dealing with arrays\n",
        "import os                  # dealing with directories\n",
        "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__ZZSCZqlz6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DIR = '/content/drive/My Drive/Deep_Learning_course_Capstone_Project/train'\n",
        "TEST_DIR = '/content/drive/My Drive/Deep_Learning_course_Capstone_Project/test'\n",
        "IMG_SIZE = 224\n",
        "LR = 1e-3\n",
        "\n",
        "MODEL_NAME = 'pet_classifier_2_Conv_basic'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDPUw_7WnbaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_img(folder_name):\n",
        "    \n",
        "    # conversion to one-hot array [cat,dog]\n",
        "    #                            [much cat, no dog]\n",
        "    if folder_name == 'cats': return [1,0]\n",
        "    #                             [no cat, very doggo]\n",
        "    elif folder_name == 'dogs': return [0,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQovP4u-nvJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_data():\n",
        "    training_data = []\n",
        "    for n,folder in enumerate(os.listdir(TRAIN_DIR)):\n",
        "      images = os.listdir(os.path.join(TRAIN_DIR,folder))\n",
        "      for i,image in enumerate(images):\n",
        "        label = label_img(folder)\n",
        "        path = os.path.join(TRAIN_DIR,folder,image)\n",
        "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
        "        training_data.append([np.array(img),np.array(label)])\n",
        "    shuffle(training_data)\n",
        "    np.save('train_data.npy', training_data)\n",
        "    return training_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI-0VoThqa5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = create_train_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N1UFwctqnqU",
        "colab_type": "code",
        "outputId": "c84b1897-c090-4ee1-e962-f121e410a209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn-mZZ_CqsX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_test_data():\n",
        "    test_data = []\n",
        "    for n,folder in enumerate(os.listdir(TEST_DIR)):\n",
        "      images = os.listdir(os.path.join(TEST_DIR,folder))\n",
        "      for i,image in enumerate(images):\n",
        "        label = label_img(folder)\n",
        "        path = os.path.join(TEST_DIR,folder,image)\n",
        "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
        "        img = img/255\n",
        "        test_data.append([np.array(img),np.array(label)])\n",
        "    shuffle(test_data)\n",
        "    np.save('test_data.npy', test_data)\n",
        "    return test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5sKdmbWr7HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = create_test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_E_lpLBQimk",
        "colab_type": "text"
      },
      "source": [
        "# Model Specification\n",
        "The CNN model (cnn_model_fn) should have the following layers: ● Input layer ● Convolutional layer 1 with 32 filters of kernel size[5,5] ● Pooling layer 1 with pool size**[2,2] **and stride 2 ● Convolutional layer 2 with 64 filters of kernel size[5,5] ● Pooling layer 2 with pool size[2,2] and stride 2 ● Dense layer whose output size is fixed in the hyper parameter: fc_size=32 ● Dropout layer with dropout probability 0.4 Predict the class by doing a softmax on the output of the dropout lay bold text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSPcLWXbsCXM",
        "colab_type": "code",
        "outputId": "9e0835b1-1147-48e8-d8c0-2bed3decc990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "import tflearn\n",
        "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "\n",
        "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
        "\n",
        "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
        "convnet = max_pool_2d(convnet, 2,strides = 2)\n",
        "\n",
        "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
        "convnet = max_pool_2d(convnet, 2,strides = 2)\n",
        "convnet = fully_connected(convnet, 32, activation='relu')\n",
        "convnet = dropout(convnet, 0.6)\n",
        "\n",
        "convnet = fully_connected(convnet, 2, activation='softmax')\n",
        "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
        "\n",
        "model = tflearn.DNN(convnet, tensorboard_dir='log')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 17:00:29.585536 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0629 17:00:29.587574 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0629 17:00:29.598469 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0629 17:00:29.608178 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0629 17:00:29.618701 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0629 17:00:29.620076 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "W0629 17:00:29.629390 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/layers/core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0629 17:00:29.632694 140685041657728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tflearn/initializations.py:119: calling UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0629 17:00:29.634409 140685041657728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
            "W0629 17:00:29.661421 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/layers/conv.py:552: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0629 17:00:29.685291 140685041657728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tflearn/initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0629 17:00:29.710855 140685041657728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tflearn/layers/core.py:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0629 17:00:29.751301 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0629 17:00:29.762521 140685041657728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0629 17:00:29.959034 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0629 17:00:30.011872 140685041657728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0629 17:00:30.383671 140685041657728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNiKjiNzuMYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = X\n",
        "test = Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODcUJ-s-s5_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
        "Y = [i[1] for i in train]\n",
        "\n",
        "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
        "test_y = [i[1] for i in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_JtM92HuSWq",
        "colab_type": "code",
        "outputId": "a999a59a-75b6-4255-b8a9-3075eb64c41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15254
        }
      },
      "source": [
        "model.fit({'input': X}, {'targets': Y}, n_epoch=300, validation_set=({'input': test_x}, {'targets': test_y}), \n",
        "    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: pet_classifier_2_Conv_basic\n",
            "Log directory: log/\n",
            "---------------------------------\n",
            "Training samples: 40\n",
            "Validation samples: 20\n",
            "--\n",
            "Training Step: 1  | time: 3.007s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 2.12332 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m7.66813\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 002 | loss: 7.66813 - acc: 0.4275 | val_loss: 3.91183 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m10.81387\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 003 | loss: 10.81387 - acc: 0.4868 | val_loss: 5.66684 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m11.33816\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 004 | loss: 11.33816 - acc: 0.4967 | val_loss: 7.38792 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m11.45915\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 005 | loss: 11.45915 - acc: 0.4990 | val_loss: 9.01594 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m11.49372\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 006 | loss: 11.49372 - acc: 0.4996 | val_loss: 10.30390 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m11.50524\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 007 | loss: 11.50524 - acc: 0.4999 | val_loss: 10.95290 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m11.50957\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 008 | loss: 11.50957 - acc: 0.4999 | val_loss: 11.17657 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m11.51134\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 009 | loss: 11.51134 - acc: 0.5000 | val_loss: 11.25890 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m11.51213\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 010 | loss: 11.51213 - acc: 0.5000 | val_loss: 11.33645 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m11.51251\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 011 | loss: 11.51251 - acc: 0.5000 | val_loss: 11.40929 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m11.51270\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 012 | loss: 11.51270 - acc: 0.5000 | val_loss: 11.47751 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m11.51279\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 013 | loss: 11.51279 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m11.51285\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 014 | loss: 11.51285 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m11.51288\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 015 | loss: 11.51288 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m11.51290\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 016 | loss: 11.51290 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m11.51291\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 017 | loss: 11.51291 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m11.51291\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 018 | loss: 11.51291 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m11.51292\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 019 | loss: 11.51292 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m11.51292\u001b[0m\u001b[0m | time: 1.111s\n",
            "| Adam | epoch: 020 | loss: 11.51292 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m11.51292\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 021 | loss: 11.51292 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m11.51292\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 022 | loss: 11.51292 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m11.51292\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 023 | loss: 11.51292 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m11.51292\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 024 | loss: 11.51292 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m11.51292\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 025 | loss: 11.51292 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 026 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 027 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 028 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 029 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 030 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 031 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 032 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 033 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 034 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 035 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 036 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 037 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 038 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 039 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.124s\n",
            "| Adam | epoch: 040 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 041 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 042 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 043 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 044 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 045 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 046 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 047 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 048 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 049 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 050 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 051 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 052 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 053 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 054 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 055 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 056 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 057 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 058 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 059 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 060 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 061 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 062 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 063 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 064 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 065 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 066 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 067 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 068 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 069 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 070 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 071 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 072 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 073 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 074 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 075 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 076 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 077 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 078 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 079 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 080 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 081 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 082 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 083 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 084 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 085 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 086 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 087 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 088 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 089 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 090 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 091 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 092 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 093 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 094 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 095 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 096 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 097 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 098 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 099 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 100 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 101 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 102 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 103 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 104 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 105 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 106 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 107 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 108 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 109 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 110 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 111 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 112 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 113 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 114 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 115 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 116 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 117 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 118 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 119 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 120 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 121 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 122 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 123 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 124 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 125 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 126 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 127 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 128 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 129  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 129 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 130  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 130 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 131  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 131 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 132 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 133  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.123s\n",
            "| Adam | epoch: 133 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 134  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 134 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 135  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 135 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 136 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 137  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 137 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 138  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 138 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 139  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 139 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 140 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 141  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 141 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 142  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 142 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 143  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 143 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 144 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 145  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 145 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 146  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 146 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 147  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 147 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 148 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 149  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 149 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 150  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 150 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 151  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 151 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 152 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 153  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 153 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 154  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 154 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 155  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 155 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 156 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 157  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 157 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 158  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 158 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 159  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 159 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 160 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 161  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 161 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 162  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 162 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 163  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 163 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 164 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 165  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 165 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 166  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 166 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 167  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 167 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 168 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 169  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 169 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 170  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 170 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 171  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 171 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 172 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 173  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 173 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 174  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 174 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 175  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 175 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 176 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 177  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 177 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 178  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 178 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 179  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 179 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 180 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 181  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 181 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 182  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 182 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 183  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 183 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 184 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 185  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 185 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 186  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 186 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 187  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 187 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 188 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 189  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 189 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 190  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 190 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 191  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 191 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 192 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 193  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 193 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 194  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 194 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 195  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 195 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 196 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 197  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 197 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 198  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 198 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 199  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 199 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 200 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 201  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 201 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 202  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 202 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 203  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 203 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 204  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 204 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 205  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 205 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 206  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 206 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 207  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 207 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 208  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 208 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 209  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 209 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 210  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.121s\n",
            "| Adam | epoch: 210 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 211  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 211 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 212  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 212 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 213  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 213 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 214  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 214 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 215  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 215 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 216  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 216 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 217  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.119s\n",
            "| Adam | epoch: 217 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 218  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 218 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 219  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 219 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 220 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 221  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 221 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 222  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 222 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 223  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 223 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 224  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 224 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 225  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 225 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 226  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 226 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 227  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.111s\n",
            "| Adam | epoch: 227 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 228  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 228 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 229  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 229 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 230  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 230 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 231  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 231 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 232  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 232 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 233  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 233 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 234  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 234 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 235  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 235 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 236  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 236 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 237  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 237 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 238  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 238 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 239  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 239 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 240  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 240 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 241  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 241 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 242  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 242 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 243  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 243 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 244  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 244 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 245  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 245 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 246  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 246 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 247  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.124s\n",
            "| Adam | epoch: 247 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 248  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 248 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 249  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 249 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 250  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 250 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 251  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 251 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 252  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 252 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 253  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 253 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 254  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 254 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 255  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 255 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 256  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 256 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 257  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 257 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 258  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 258 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 259  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 259 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 260  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 260 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 261  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 261 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 262  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 262 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 263  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 263 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 264  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 264 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 265  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 265 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 266  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 266 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 267  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 267 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 268  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 268 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 269  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 269 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 270  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 270 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 271  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 271 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 272  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 272 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 273  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.121s\n",
            "| Adam | epoch: 273 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 274  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 274 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 275  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 275 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 276  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 276 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 277  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 277 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 278  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 278 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 279  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 279 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 280  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 280 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 281  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 281 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 282  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 282 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 283  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 283 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 284  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.112s\n",
            "| Adam | epoch: 284 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 285  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 285 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 286  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 286 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 287  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 287 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 288  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 288 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 289  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 289 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 290  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 290 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 291  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 291 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 292  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 292 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 293  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 293 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 294  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 294 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 295  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 295 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 296  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.116s\n",
            "| Adam | epoch: 296 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 297  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.113s\n",
            "| Adam | epoch: 297 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 298  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.117s\n",
            "| Adam | epoch: 298 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 299  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.114s\n",
            "| Adam | epoch: 299 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n",
            "Training Step: 300  | total loss: \u001b[1m\u001b[32m11.51293\u001b[0m\u001b[0m | time: 1.115s\n",
            "| Adam | epoch: 300 | loss: 11.51293 - acc: 0.5000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 40/40\n",
            "--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjEO-3KUusu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/Deep_Learning_course_Capstone_Project/{}'.format(MODEL_NAME))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etck2Q6VDUP4",
        "colab_type": "code",
        "outputId": "4b41ad8d-6e48-448b-e1c2-8723fbf6b3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxddn5sJDkRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}